var documenterSearchIndex = {"docs":
[{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Parallel-map","page":"Examples","title":"Parallel map","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Distributed\n\naddprocs(5)\n@everywhere using Distributed, Schedulers\n@everywhere function foo(tsk)\n    @info \"sleeping for task $tsk on worker $(myid()) for 60 seconds\"\n    sleep(60)\nend\n\nepmap(foo, 1:20)\nrmprocs(workers())","category":"page"},{"location":"examples/#Parallel-map-reduce","page":"Examples","title":"Parallel map reduce","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"The parallel map method epmapreduce! creates memory on each worker process for storing a local reduction.  Upon exhaustion of the tasks, the local reductions are reduced into a final reduction.  The memory allocated for each worker is dictated by the first argument to epmapreduce! which is also the memory that holds the final reduced result.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Distributed\n\naddprocs(5)\n@everywhere using Distributed, Schedulers\n@everywhere function foo(x, tsk)\n    localpart(x) .+= tsk\n    @info \"sleeping for task $tsk on worker $(myid()) for 60 seconds\"\n    sleep(60)\nend\nx = epmapreduce!(zeros(Float64,5), foo, 1:10) # x=[10,10,10,10,10]","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Note that x in the above examples is a ArrayFutures which is defined in the (DistributedOperations.jl)[https://github.com/ChevronETC/DistributedOperations.jl] package.","category":"page"},{"location":"examples/#Parallel-map-with-elasticity","page":"Examples","title":"Parallel map with elasticity","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Both pmapreduce and epmapreduce can accept epmap_minworkers and epmap_maxworkers key-word arguments to control the elasticity.  In addition the epmap_quantum and epmap_addprocs arguments are used to control how workers are added.  Distributed.rmprocs is used to shrink the number of workers.  For example, the number of workers will shrink when the number of remaining tasks is less than the number of Julia workers, or when a worker is deemed not usable due to fault handling.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Distributed\n\naddprocs(10)\n@everywhere using Distributed, Schedulers\n@everywhere function foo(tsk)\n    @info \"sleeping for task $tsk on worker $(myid()) for 60 seconds\"\n    sleep(60)\nend\n\nepmap(foo, 1:20; emap_minworkers=5, emap_maxworkers=15, emap_addprocs=addprocs)\nrmprocs(workers())","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Note that as epmap elastically adds workers, methods and packages that are defined in Main will automatically be load on the added workers.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [DistributedOperations]\nOrder   = [:function, :type]","category":"page"},{"location":"#Schedulers.jl","page":"Schedulers.jl","title":"Schedulers.jl","text":"","category":"section"},{"location":"","page":"Schedulers.jl","title":"Schedulers.jl","text":"Shedulers.jl provides elastic and fault tolerant parallel map (epmap) and parallel map reduce methods (epmapreduce!).  The primary feature that distinguishes Schedulers parallel map method from Julia's Distributed.pmap is elasticity where the cluster is permitted to dynamically grow/shrink. The parallel map reduce method also aims for features of fault tolerance, dynamic load balancing and elasticity.","category":"page"},{"location":"","page":"Schedulers.jl","title":"Schedulers.jl","text":"This package can be used in conjunction with AzSessions.jl, AzStorage.jl and AzManagers.jl to have reasonable parallel map and parallel map reduce methods that work with Azure cloud scale-sets. The fault handling and elasticity allows us to, in particular, work with Azure cloud scale-sets that are using Azure Spot instances.","category":"page"},{"location":"#Implementation-details","page":"Schedulers.jl","title":"Implementation details","text":"","category":"section"},{"location":"","page":"Schedulers.jl","title":"Schedulers.jl","text":"The current implementation of the parallel map reduce method uses shared storage (e.g. NFS, Azure Blob storage...) to check-point local reductions.  This check-pointing is what allows for fault tolerance, and to some extent, elasticity.  However, it is important to note that it comes at the cost of IO operations.  It would be useful to investigate alternatives ideas that avoid IO such as resilient distributed data from Spark.","category":"page"},{"location":"#Important-note","page":"Schedulers.jl","title":"Important note","text":"","category":"section"},{"location":"","page":"Schedulers.jl","title":"Schedulers.jl","text":"Note that the error handling in the epmapreduce method is, as of Schedulers 0.1.0, limited to the case where a worker is removed, throwing a ProcessExitedException.  In future versions, we will work to expand the scope of the error handling.","category":"page"}]
}
